
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="content-Type" CONTENT="text/html; charset=iso-8859-1">
<link rel="StyleSheet" href="./css/jboost.css" type="text/css">
<title>
JBoost
</title>
</head>
<body>

<div class="headnav">
 <a href="./index.html"><img src="./images/JBoostLogo1_IE.png" alt="JBoost Logo" /></a>
 <div class="headmenu">
  | <a class="headanchor" href="./index.html">Home</a> |
  <a class="headanchor" href="./downloads.html">Downloads</a> |
  <a class="headanchor" href="./doc.html">Documentation</a> |
  <a class="headanchor" target="_blank" href="http://jboost.wiki.sourceforge.net/FAQ">FAQ</a> |
  <a class="headanchor" href="./publications.html">Publications</a> |
 </div>
</div>


<table class="main_table">

<tr>

<td align="left" valign="top">
<table class="sidebar">

<tr class="sidebar_header"><td>
General
</td></tr>

<tr><td>
<a href="./index.html">Home</a>
</td></tr>
<tr><td>
<a href="./downloads.html">Downloads</a>
</td></tr>
<tr><td>
<a href="./about.html">Mailing List</a>
</td></tr>


<tr class="sidebar_header"><td>
Getting Started
</td></tr>

<tr><td>
<a href="./install.html">Installing</a>
</td></tr>
<tr><td>
<a href="./tutorial.html">Tutorial</a>
</td></tr>
<tr><td>
<a href="./tips.html">Tips</a>
</td></tr>
<tr><td>
<a href="./doc.html">Documentation</a>
</td></tr>
<tr><td>
<a href="./examples.html">Examples</a>
</td></tr>
<!--
<tr><td>
<a href="./screenshots.html">Screenshots</a>
</td></tr>
-->
<tr><td>
<a href="http://jboost.wiki.sourceforge.net/FAQ" target="_blank">FAQ</a>
</td></tr>


<tr class="sidebar_header"><td>
Developers
</td></tr>

<!--
<tr><td>
<a href="./contribute.html">Developers Mailing List</a>
</td></tr>
-->
<tr><td>
<a href="http://sourceforge.net/projects/jboost/">SourceForge Page</a>
</td></tr>

</table>

</td>

<td  align="left" valign="top">
<!#if expr="${include_file}" -->
  
<div align="left" class="section">
<h2 class="section_heading" >Documentation</h2>
<p>
JBoost has several boosting algorithms, and each algorithm has its own
set of parameters.  There are several forms of documentation
available:
</p>
<ul>

<li> <a href="#basics">ADTrees and Boosting</a></li>

<li> <a href="./install.html">Installing</a></li>

<li><a href="#input">Input Files</a></li>

<li><a href="#output">Output Files</a></li>

<li> <a href="#algs">Usage Options</a></li>

<li><a href="#results">Using Code Output</a></li>

<li><a href="serializedTree.html">Save and Load Partially Trained Classifier</a></li>

<li><a href="./scoreVisualizer.html">The Score Visualizer</a></li>

<li><a href="#visualization">Other Visualization Tools</a></li>

<li><a href="#cv">Cross-Validation</a></li>

<li><a href="#multiclass">Multiclass Problems</a></li>

<li><a href="#rbsearch">RobustBoost Parameter Search</a></li>

<li><a href="#boost_format"><code>boosting.info</code> Format</a></li>

<li><a href="./tips.html">Tips</a>.  A short list of simple tricks for
getting better results from JBoost. </li>

<li><a href="#links">Boosting</a>.  Learn more about the theory
behind boosting. </li>


</ul>
</div>


<div align="left" class="section">
<h2 class="section_heading" ><a name="basics">What are
ADTrees?</a></h2> 

<p>ADTrees are a generalization of decision trees that utilize the
boosting model for machine learning.  There is a <a
href="http://en.wikipedia.org/wiki/Alternating_decision_tree">Wikipedia
ADTree page</a> that describes how to read the data structure and how
the construction algorithm works.  There is also a <a
href="./presentations/BoostingLightIntro.pdf">short presentation
(PDF)</a> describing the data structure with many graphical examples.
For more information on boosting, see the <a href="#links">links
below</a>.</p> </div>



<div align="left" class="section">
<h2 class="section_heading" ><a name="input">Input Files</a></h2>

<p>JBoost requires three data files for the data specification,
training data, and test data.  There are several examples in the
<code>demo</code> directory of the distribution.
</p>

<p>The data specification file contains definitions for the
configurable parameters, the types of each example attribute and the
possible set of labels for each example. A sample data specification
looks like this,
</p>

<pre>
 exampleTerminator=;
 attributeTerminator=,
 maxBadExa=0 
 INDEX            number
 age              number
 income           number
 sex              (male, female)
 highest-degree   (none, high-school, bachelors, masters, phd)
 goal-in-life     text
 labels           (rich, smart, happy, none)
</pre>


<p>
This format lists the configurable parameters, followed by the
attribute definitions, and the possible labels. The parameters are
summarized, 
</p>

<table  class="inset">
<caption>
Spec File Parameters
</caption>
<tr><th> 
Parameter Name
</th><th>
Description
</th><th>
Possible Values
</th><th>
Optional?
</th>
</tr><tr>
<td> <code>exampleTerminator</code> </td><td> the character used to signify the end of each example </td><td> a typical value is ';' </td><td> Required</td>
</tr><tr>
<td> <code>attributeTerminator</code> </td><td>  the character used to separate attributes of each example </td><td> a typical value is ',' </td><td> Required</td>
</tr><tr>
<td> <code>maxBadExa</code> </td><td> the maximum number of bad examples that will be tolerated </td><td> default = 20 </td><td> Optional </td>
</tr><tr>
<td> <code>maxBadAtt</code> </td><td> the maximum number of bad attributes that will be tolerated </td><td> default = 0</td><td> Optional </td>
</tr>
</table>


<p>
The special keyword attribute names are
</p>

<table class="inset" >
<tr><th> 
Attribute Name
</th><th>
Description
</th><th>
Possible Values
</th><th>
Optional?
</th>
</tr>
<tr>
<td> <code>INDEX</code> </td><td> the unique index number of each example</td><td> n/a </td><td> Optional <!--(Required by the score visualizer)--> </td>
</tr>
<tr>
<td> <code>weight</code> </td><td> an initial weighting of the data (higher weight implies greater importance to classify correctly)</td><td> default = 1.0 </td><td> Optional </td>
</tr>
<tr>
<td> <code>labels</code>  </td><td> the list of possible labels (Order matters! See below) </td><td> this is a <code>finite</code> type (see below) </td><td> Required </td>
</tr>
</table>

<p>
The attribute definitions include the attribute name and the type. The
attribute names must be listed in the order they appear in the data.
There are keyword attribute names: <code>weight</code> ascribes an
importance to the example and <code>labels</code> defines the label
for the example.  The order in which the labels are defined changes
any post-processing steps, so be sure you define your labels as text
(e.g. <code>labels (rich, poor)</code>) or increasing in numeric value
(e.g. <code>labels (-1,1)</code>) to get the most intuitive
post-processing results.  JBoost considers the label to be an
enumeration of values and does not take into account any numeric value
associated with such values.  While specific ordering of labels isn't
necessary, it may allow for less post-processing changes.
</p>

<p>
There is no
other special relationships between the data and the attribute names.
</p>

<p>
There are currently only three attribute types
supported in JBoost. Attributes of <code>finite</code> type must
include the set of possible values. This set is a parenthesized
comma-separated list.
</p>

<table class="inset" align="center">
<tr><th> 
Attribute Type
</th><th> Description </th>
</tr>
<tr>
<td>
 <code>number</code> 
</td><td>
 the attribute can be any number, represented as a floating point number 
</td>
</tr><tr>
<td>
 <code>finite</code> 
</td><td> the attribute is a member of a defined set of strings 
</tr><tr>
<td> 
 <code>text</code> 
</td><td>
 the attribute is a sequence of words delimited by whitespace 
</td></tr>
</table>

<!--
<p>
Attribute types may also have options. These options allow customization of data processing.
</p>

<table class="inset" >
<tr><th><strong> Attribute Type Option </strong></th>
<th><strong> Description </strong></th>
<th><strong> Possible values </strong></th>
<th><strong> Applicable Types </strong></th>
</tr>
<tr><td>  <code>ignoreCase</code>  </td>
<td>  convert everything to lowercase  </td>
<td>  {+, -}  </td>
<td>  <code>text</code> defaults to +, <code>finite</code> defaults to -  </td>
</tr>
<tr><td>  <code>ignorePunctuation</code>  </td>
<td>  omit all punctuations and replace multiple whitespace with a single space  </td>
<td>  {+, -}  </td>
<td>  <code>text</code> defaults to +, <code>finite</code> defaults to -  </td>
</tr>
<tr><td>  <code>ignoreAttribute</code>  </td>
<td>  ignore this attribute during the learning process  </td>
<td>  {true, false}  </td>
<td>  applies to all attribute types  </td>
</tr>
<tr><td>  <code>ngramsize</code>  </td>
<td>  maximum number of consecutive elements  </td>
<td>  some integer  </td>
<td>  <code>text</code> only <strong>not implemented</strong>  </td>
</tr>
<tr><td>  <code>ngramtype</code>  </td>
<td>  type of ngrams to use  </td>
<td>  <code>fixed</code> (size of <code>ngramsize</code>), <code>full</code> (size up to <code>ngramsize</code>), <code>sparse</code>  </td>
<td>  <code>text</code> only  <strong>not implemented</strong>  </td>
</tr>
<tr><td>  <code>multiLabel</code>  </td>
<td>  allow multiple labels to be assigned to an example  </td>
<td>  {true, false}  </td>
<td>  <code>labels</code> only, defaults to false   <strong>not implemented</strong>  </td>
</tr></table>
-->

<p>
After you have completed the data specification you have to update the
test data and training data to follow the definition in the
specification file. Lines starting with '//' in the data files are
ignored. Each line should contain an example with attributes separated
by the <code>attributeTerminator</code>. Each line should end with the
<code>exampleTerminator</code>. If one of the terminators needs to be
used as part of the data, it can be 'escaped' using the '\'
character. An example test file, using the example specification
above, looks like this,
</p>

<pre> 
 51, 52000, male, phd, be with my family, 1.0, happy;
 24, 1000000, male, bachelors, retire at 25, 0.5, rich; 
</pre>

<p>
After you have completed creating your train and test files, you can
use JBoost to learn an alternating decision tree.
</p>

</div>

<div align="left" class="section">
<h2 class="section_heading" ><a name="output">Output Files</a></h2>

<p>
There are many options that will create different types of output
files.  To turn on/off certain output,  see the <a href="#algs">Usage
Details</a> section.  This section provides an overview of all the
options.
</p>

<ul>

<li> Option and error information.  A very high level file gives all
options used to learn the ATree and the error rate at every
iteration.</li>

<li> Boosting Information.  Boosting methods have weights, margins,
and potential loss associated with each example on each iteration. These files are useful for visualizing score/margin distribution.
These files may grow to be very large (100s MBs) with large datasets
and many iterations.  Use the <code>-a</code> switch to change the
amount of output.</li>

<li> Classifier code.  For many of the boosters, a classifier can be
output to a java, C++, python, or matlab code file.  This can then be
integrated into your on project or used from the command line.  </li>

<li> ATree.  This is a text file that is human readable and PERL
parsable (see <code>atree2dot2ps.pl</code>).  It is useful for
visualization, but cannot be used to classify future inputs (unless
you want to implement that feature!).  </li>

<li> Serial Atree.  These trees can be read in by java at a later
date.  This option is useful if you have a large and complicated
classifier that may take a long time to learn.</li>

</ul>

</div>

<!--
<div align="left" class="section">
<h2 class="section_heading" ><a name="#algs">Usage</a></h2>

<p>
Perhaps the most valuable documentation is the <a
href="./examples.html">list of examples</a>.  These examples
demonstrate the most common use of JBoost and tools.
</p>

<p>
Here is a full list of options that can be used.
</p>

<pre>
jboost Version 1.2

** Config Options:

        -p N       Specify number of threads (default: 1)
        -CONFIG    The name of the configuration file (default "jboost.config")
                   All options (below) can be specified in this file.
        -V         Print version and exit

** Data File Options:

        -S stem        Base (stem) name for the files (default: "data")
        -n file.spec   Specfile name (default: stem+".spec")
        -t file.train  Training file name (default: stem+".train")
        -O file.tree   Output tree file name (default: stem+".output.tree")

** Boosting Options:

        -b type   The type of booster to use (default: AdaBoost).
                  AdaBoost     Loss function exp(-margin)
                  LogLossBoost Loss: log ( 1 + exp(-margin) )
                  BrownBoost   Loss: 1/2 - 1/2 * erf((-margin + time_remaining) 
		                                   / sqrt(total_time))
                  YabaBoost    Loss: see documentation (it's a tad complicated...).
        -numRounds N  The number of rounds of boosting that are to be executed.
                      This option should be used with AdaBoost and LogitBoost
        -r c       The "runtime" of the boosting game.
                   This option should be used with BrownBoost and YabaBoost
        -booster_paranoid Use safe version of booster (default: false)
        -booster_smooth sf   Smoothing factor for prediction computation (default: 0.5)
                             Described Schapire & Singer 1999 (smoothing the predictions),
                             $epsilon = sf / total_num_examples$
        -BoosTexter        Only make a zero prediction at the root node.
        -ATreeType type   The type of ATree to create.  There are several options:
                          ADD_ALL               Create a full ADTree (default)
                          ADD_ROOT              Add splits only at the root producing a glat tree.
                                                This is equivalent to boosting decision stumps
                          ADD_SINGLES           Create a decision tree
                          ADD_ROOT_OR_SINGLES   Create a linear combination of decision trees.
                                                This is equivalent to simultaneously growing
                                                boosted decision trees.

** Code Output Options:

        -j filename    Output java code file name (default: stem+".output.java"
        -c filename    Output C code file name (default: stem+".output.c")
        -m filename    Output matlab code file name (default: stem+".output.java"
        -cOutputProc name  Name of procedure for output C code (default: 'predict')
        -javaStandAlone    Output java code that can stand alone, but
                           cannot read jboost-format data
        -javaOutputClass name     Name of class for output java code (default: 'Predict')
        -javaOutputMethod name    Name of method for output java code (default: 'predict')

** Logging Options:

        -info filename      High-level log file name (default: stem+".info")
        -log  filename      Debugging log (default stem+".log")
        -loglevel N   Amount of information to be output to log
                      The larger N is, the more information will be output.
                      This is meant to be used as a debugging tool.
        -a iter      Generate margin (score) logs
                     iter>0   log only on iteration iter,
                     iter=-1  log on iters 1,2..9,10,20,...,90,100,200 ...)
                     iter=-2  log on all iterations
</pre>
</div>
-->











<div align="left" class="section">
<h2 class="section_heading" ><a name="algs">Usage Details</a></h2>

<p>
There are large number of usage options.  While there are many
options, the most common runs are shown in a <a
href="./examples.html">list of examples</a>.
</p>



<hr />



<p>
The global configuration options are:
</p>
<pre>
        -p N       Specify number of threads (default: 1)
        -CONFIG    The name of the configuration file (default "jboost.config")
                   All options can be specified in this file instead of on 
		   the command line.
        -V         Print version and exit
</pre>
<p>
If you have a multiprocessing environment, the <code>-p N</code>
switch will tell JBoost how many threads to use.  By default,
JBoost only uses one thread.  JBoost spends most of it's time
searching for good weak hypotheses.  This scales inverse-linearly to
the number of threads.  <br /> The <code>-CONFIG filename</code>
switch specifies a configuration file to use instead of the command
line parameters.  If you find yourself using many parameters, it may
be easier to specify all of them in a file.  If you use a Windows
system and don't want to use the command line, you can create
jboost.config with all the parameters specified inside.
An example configuration file could be:
</p>
<pre>
-n src/jboost/controller/data.spec
-T src/jboost/controller/data.test
-t src/jboost/controller/data.train
-a -1
-serialTreeOutput src/jboost/controller/atree.serialized
</pre>

<hr />


<p>
Data/Test/Train file options are:
</p>
<pre>
        -S stem        Base (stem) name for the files (default: "data")
        -n file.spec   Specfile name (default: stem+".spec")
        -t file.train  Training file name (default: stem+".train")
        -T file.test   Test file name (default: stem+".test")
        -serialTreeInput file.tree   Java object of adtree (default: stem+".output.tree")
        -weightThreshold T    Set threshold for accepting an example
</pre>
<p>
The <code>-S stem</code> switch is the most common way to specify the
train/test/spec file.  If you use various non-standardized filenames,
you can specify the spec, train, and test files separately.  The
serialized input tree is a Java Object of an ADTree that was outputted
by a previous run of JBoost. Moreover, when the serialized input tree is 
given, you have an option to specify a weight theshold by using 
<code>-weightThreshold T</code> so that any examples with weight smaller 
than <code>T</code> will be ignored. This can be very useful when the dataset is large. 
</p>


<hr />


<p>
Boosting options:
</p>
<pre>
    -b type   The type of booster to use (default: AdaBoost).
              AdaBoost     Loss function: exp(-margin)
              LogLossBoost Loss: log(1 + exp(-margin))
              RobustBoost  Loss: min(1,1-erf((margin - mu(time))/sigma(time)))
    -numRounds N  The number of rounds of boosting that are to be executed.
                  This option should be used with AdaBoost and LogLossBoost
    -ATreeType type   The type of ATree to create.  There are several options:
                      ADD_ALL               Create a full ADTree (default)
                      ADD_ROOT              Add splits only at the root producing a glat tree.
                                            Equivalent to boosting decision stumps
                      ADD_SINGLES           Create a decision tree
                      ADD_ROOT_OR_SINGLES   Create an ensemble combination of decision trees.
    -BoosTexter        Only make a zero prediction at the root node.
    -booster_smooth sf   Smoothing factor for prediction computation (default: 0.5)
                         Described Schapire & Singer 1999 (smoothing the predictions),
                         $epsilon = sf / total_num_examples$
    -booster_paranoid Use debugging version of booster (default: false)
</pre>
<p>
The most common usage is <code>-b AdaBoost -numRounds 100</code>.  If
test error doesn't asymptote (see <a
href="#visualization">visualizations</a>) run for more rounds.  If
AdaBoost is overfitting use LogLossBoost and/or
BrownBoost. Descriptions can be found on Wikipedia for <a
href="http://en.wikipedia.org/wiki/LogitBoost">LogLossBoost
(LogitBoost)</a> and RobustBoost boosting
algorithms.  
</p>

<p>
The type of alternating decision tree can drastically changed the
classifier.  Depending on the dataset, one type of tree may be better
than the others.  However, the ADD_ALL type is the most flexible.
</p>

<p>
BoosTexter is an implementation of Schapire and Singer's bag of words
method where zero predictions are allowed at the root node.
</p>


<hr />


<p>
RobustBoost options:
</p>

<pre>
    -rb_time       NUM          See documentation.
    -rb_epsilon    NUM          See documentation.
    -rb_theta      NUM          See documentation.
    -rb_theta_0    NUM          See documentation.
    -rb_theta_1    NUM          See documentation.
    -rb_sigma_f    NUM          See documentation.
    -rb_sigma_f_0  NUM          See documentation.
    -rb_sigma_f_1  NUM          See documentation.
    -rb_cost_0     NUM          See documentation.
    -rb_cost_1     NUM          See documentation.
    -rb_conf_rated <true|false> See documentation.
    -rb_potentialSlack   NUM    See documentation.
</pre>
<p>
The RobustBoost option ... TODO
</p>


<hr />


<p>
Code Output Options:
</p>
<pre>
        -serialTreeOutput file.tree    Java object output of adtree
        -O file.tree   Output tree file name (default: stem+".output.tree")
        -P filename    Output python code file name 
        -j filename    Output java code file name 
        -c filename    Output C code file name 
        -m filename    Output matlab code file name 
        -cOutputProc name  Name of procedure for output C code (default: 'predict')
        -javaStandAlone    Output java code that can stand alone, but
                           cannot read jboost-format data
        -javaOutputClass name     Name of class for output java code (default: 'Predict')
        -javaOutputMethod name    Name of method for output java code (default: 'predict')
</pre>
<p>
There are two ways to output the ADTree: a text format meant for
visualization or a code format meant for classification.  The code
can be integrated with JBoost or a custom classification system.
</p>
<p>
Make sure not to forget to specify the <code>-j</code>,
<code>-c</code>, or <code>-m</code> flags if you wish to have code
output.  Also remember that the Java class name must match the
filename (a Java constraint).
</p>


<hr />


<p>
Logging Options
</p>
<pre>
        -info filename      High-level log file name (default: stem+".info")
        -log  filename      Debugging log (default stem+".log")
        -loglevel N   Amount of information to be output to log
                      The larger N is, the more information will be output.
                      This is meant to be used as a debugging tool.
        -a iter      Generate margin (score) logs
                     iter>0   log only on iteration iter,
                     iter=-1  log on iters 1,2..9,10,20,...,90,100,200 ...)
                     iter=-2  log on all iterations
                     iter=-3  log only on the last iteration of boosting
</pre>
<p>
To obtain the margin distribution, the <code>-a iter</code> option
must be used.  The <code>loglevel</code> is meant to be used by
developers for debugging purposes.  The default values for
<code>-info</code> and <code>-log</code> are typically sufficient.
</p>

</div>


<div align="left" class="section">
<h2 class="section_heading" ><a name="results">Using JBoost Code Output</a></h2>
<p>
JBoost generates several files containing information about the run.
These files can be categorized into two groups: log files and classifier files.
</p>

<p>
There are two log files. The file <code>stem.info</code> is generated
as the program runs.  It contains a high level log, containing
information such as error rate for given iteration and command line
arguments.  The other log file is <code>stem.log</code>, which
contains a detailed log.  The granularity of the log is controlled by
the <code>-logLevel</code> switch.  Large values of N should only be
used by developers trying to debug.
</p>

<p>
There can be numerous classifier files.  The generated ADTree will be
stored in <code>stem.output.tree</code>.  If you use JBoost to create
source code for the classifier (via one of the switches <code>-j, -c,
-m</code>), then code representing this tree will be generated in
files with those names.  We assume here that the defaults are used for
the Java and C code output.
</p>

<hr />

<p>
The Python code contains a single class (ATree) and a main function
demonstrating how to use the class.  The classifier file (for example
spambase.py) can be used on a new dataset (spambase.data) using a
format specified in a specfile (spambase.spec) via
</p>
<pre>
    $> python spambase.py spambase.data spambase.spec
</pre>

<p>
To write your own code, see the <code>main</code> function at the
bottom of the file.
</p>

<hr />

<p>
The C code contains a single procedure:
</p>
<p>
<code>
double predict(void **attr, double *ret)
</code>
</p>
<p>
The first argument <code>attr</code> is an array of pointers
corresponding to the attributes specified in the spec file.  Thus, if
attribute i is text, then <code>attr[i]</code> must be a char array; if
attribute i is a number, then <code>*attr[i]</code> must be a double;
and if attribute i is finite, then <code>*attr[i]</code> must be an
int containing the index of the chosen value.  If attribute i is
undefined then attr[i] should be set to NULL.
</p>
<p>
The second argument <code>ret</code> is a pointer to an array of k
doubles, where k is the number of classes.  The scores for each of the
k classes will be stored in this array.  If ret is NULL, then no
scores are stored.  In any case, predict returns the score for class 0
(ret[0]).
</p>

<hr />

<p>
The .java file contains a class called 'Predict'.  To use this file,
it must be moved to a file called Predict.java.  This file can be
compiled using
</p>
<p>
<code>
javac Predict.java
</code>
</p>
<p>
Unless the javaStandAlone command line option is invoked, this file
can be run (assuming you are in the same directory as Predict.java)
using
</p>
<p>
<code>
java Predict < datafile
</code>
</p>
<p>
If you want to call Predict from an alternative directory, set an
environment variable to the directory of Predict.java as such,
</p>
<p>
<code>
PREDICTPATH="/dir/to/predict"
<br />
java -cp "$CLASSPATH:$PREDICTPATH" Predict < datafile
</code>
</p>
<p>
In this mode, the program reads examples from standard input having
the identical form of examples contained in the .train and .test
files.  After each example is read, a vector of scores (one for each
class) is output to standard output.
</p>
<p>
Alternatively, the method
</p>
<p>
<code>
static public double[] predict(Object[] attr)
</code>
</p>
<p>
can be called from another java program.  The argument attr is an
array of Objects corresponding to the attributes specified in the spec
file.  Thus, if attribute i is text, then attr[i] must be a String; if
attribute i is a number, then attr[i] must be a Double; and if
attribute i is finite, then attr[i] must be an Integer containing the
index of the chosen value.
</p>
<p>
The return value of this procedure is an array of doubles containing
the scores for each of the classes.
</p>
<hr />
<p>
A third option is to call the method
</p>
<p>
<code>
static public double[] predict(String[] attr)
</code>
</p>
<p>
which also can be called from another java program.  As before, the
argument attr is an array corresponding to the attributes specified in
the spec file.  Now, however, all of the values of this array are
specified by Strings.  Thus, if attribute i is text, then attr[i] is
the text string itself; if attribute i is a number, then attr[i] is a
String representing this number; and if attribute i is finite, then
attr[i] must be the chosen value itself (not its index) represented as
a String.  Note that removing punctuation, converting to lower case,
removing leading or trailing blank spaces, etc. are the responsibility
of the calling procedure.
</p>
<p>
As before, the return value of this procedure is an array of doubles
containing the scores for each of the classes.
</p>
</div>





<div align="left" class="section">
<h2 class="section_heading" ><a name="visualization">Other Visualization Tools</a></h2>
<p>
There are currently a few visualization tools.  All of these programs
depend on files outputted by JBoost.  There are currently tools for
visualizing three aspects of JBoost: 1) the error, 2) the margin, and
3) the alternating decision tree.  The tools can be found in the
scripts directory and when ran without arguments, will output their
usage.
</p>

<p>
We assume that the following programs are installed and are accessible
in your executable path:
<ul>

<li> <a href="http://www.python.org">Python</a> - Used in the
<code>*.py</code> scripts.  Available for nearly all platforms and
comes installed by default on most Linux and Mac OS X machines.</li>

<li> <a href="http://www.perl.org">PERL</a> - Used by the
<code>*.pl</code> scripts.  Available for nearly all platforms and
comes installed by default on most Linux and Mac OS X machines.  </li>

<li> <a href="http://www.graphviz.org/">Graphviz</a> - A package from
AT&amp;T labs for displaying graphs.  Used by
<code>atree2dot2ps.pl</code>. </li>

<li> <a href="http://www.gnuplot.info/">gnuplot</a> - A multi-platform
data plotting utility. Used by all the <code>*.py</code>
scripts. </li>

<li> (Optional) <a href="http://www.cygwin.com/">cygwin</a> - An
environment similar to Linux that can run on Windows machines.  JBoost
(and associated scripts) can run on Windows machines, but
<code>cygwin</code> makes it much simpler for people used to a linux
environment.  You can also use cygwin to download all the above
programs (except for Graphviz, which I believe you'll have to do
through windows).</li>

</ul>
</p>

<hr />
<p>
Visualizing the ADTree is important for understanding the intuition
for why the classifier works.  The program
<code>atree2dot2ps.pl</code> parses an <code>info</code> file and a
<code>tree</code> file to produce a ps,pdf, and png representation of
the ADTree.  This program is written in PERL and uses the <a
href="http://www.graphviz.org/">Graphviz</a> package from AT&amp;T labs.
Usage is: 
</p>
<pre>
./atree2dot2ps.pl usage: 
         -i (--info) file.info   File containing runtime information (required) 
         -t (--tree) file.tree   File containing the ADTree in text format (required) 
         -d (--dir)  directiory  Directory to use (optional, defaults to '.') 
         -l (--labels)           Flip the labels (-1 becomes +1) (optional)
         --truncate              Truncate threshold values to increase readability (optional)
         --threshold num         A given depth to stop the tree from becoming too large. (optional) 
         -h (--help)             Print this usage information 
</pre>
<p>
An example (shows the first 10 nodes of the spambase ADTree) is
</p>
<pre>
./atree2dot2ps.pl  --info spambase.info --tree spambase.output.tree --threshold 10 
</pre>
<p>
which produces (click to see full size image)
</p>
<br />
<center>
<a href="./images/spambase.10.png" > <img width="500" src="./images/spambase.10.png"  
alt="pic of spambase adtree" /></a>
</center>


<hr />
<p>
Boosting doesn't require many parameters, in fact it only really
requires one parameter: the number of iterations.  Specifying this
number is non-trivial; however, there is somewhat of a consensus that
there are a few ways to determine an appropriate number of iterations:
</p>
<ul>

<li> When <strong>test</strong> error asymptotes or begins to
increase, the number of iterations is sufficient.</li>

<li> When the margin distribution "converges" </li>

</ul>

<p>
These two conditions combined with cross validation experiments (and
boosting's proclivity to not overfit) are typically sufficient
evidence for choice of number of iterations.  There are two scripts:
<code>error.py</code> and <code>margin.py</code>.
</p>

<p>
The error.py script can be ran while JBoost is still executing.
It reads the info file which is created incrementally.  Here is the
usage for the program:
</p>
<pre>
Usage: error.py    
         --info=info_file    scores file as output by jboost
         --logaxis           should the axis be log-scaled (default: false)
         --bound             show the bound on training error
</pre>

<p>
An example (1000 iterations on the spambase ADTree) is
</p>
<pre>
./error.py --info=spambase.info --logaxis
</pre>
<p>
which results in </p>
<center>
<a href="./images/spambase_error.png" > <img width="500" src="./images/spambase_error.png"  alt="pic of spambase adtree" /></a>
</center>



<hr />

<p> margin.py can be ran while JBoost is still executing.  It reads
the given train/test file and the boosting.info file (the <code>-a
NUM</code> switch must be used while boosting), which is created
incrementally and flushed after every iteration.  Here is the usage
for the program:
</p>

<pre>
Usage: margin.py    
        --boost-info=filename  margins file as output by jboost (-a -2 switch)
        --data=filename        train/test filename
        --spec=filename        spec filename
        --iteration=i,j,k,...  the iterations to inspect, no spaces between commas
        --sample               are we interested in a sample of false pos/neg
</pre>


<p>
An example (spambase margins for 5,50,300,1000 iterations) is
</p>


<pre>
./margin.py --boost-info=spambase.train.boosting.info --data=spambase.train \
            --spec=spambase.spec --iteration=5,50,300,1000 
</pre>


<p>
which results in </p>

<center>
<a href="./images/spambase_margin.png" ><img width="500" src="./images/spambase_margin.png"  alt="pic of spambase margin" /></a>
</center>

<p>
Note that after 300 iterations, it seems that the margin distribution
has converged for the most part.  This in combination with the error
curve indicate that 300 iterations is likely to be sufficient.
However, in both cases, boosting does not overfit, thus the additional
iterations can only help.  This is not always the case though.
</p>

</div>




<div align="left" class="section">
<h2 class="section_heading" ><a name="cv">Cross Validation</a></h2>
<p>
Cross validation is a vital part of any machine learning experiment.
It demonstrates that the classifier is robust and has not been
overfitted to specific training/test data.  While CV is not 100%
conclusive, it certainly is an important piece of evidence for the
robustness of a classifier.  
</p>
<p>

The script <code>nfold.py</code> can be used to partition a dataset
into <em>k</em> folds and run JBoost on each fold.  The user gives
the script a single dataset (i.e. <em>not</em> separated
train/test files) and the script creates a directory where it creates
<em>k</em> training files and <em>k</em> test files.  Each of
the test files are disjoint and the <em>i</em>th training files
contains all the data points not found in the <em>i</em>th test
file.
</p>

<p>
Here is the usage for the program:
</p>
<pre>
Usage: nfold.py 
        --folds=N          create N files for N fold cross validation (required)
        --data=DATAFILE    DATAFILE contains the data to be divided into test/train sets (required)
        --spec=SPECFILE    SPECFILE is the same as used in JBoost typically (required)
        --rounds=N         The number of rounds to run the booster (required)
        --tree=TREETYPE    The type of tree nodes to use possible options are ADD_ROOT, 
                           ADD_ALL, ADD_SINGLES, ADD_ROOT_OR_SINGLES.  If unspecified, all
                           types will be executed sequentially
        --generate         Creates directories for the cross validation sets
        --dir=DIR          Specifies a name for the directory to be generated
</pre>

<p>
An example (5-fold CV, spambase dataset, 500 iterations, 1 tree type, test/train files and directory is generated) is
</p>
<pre>
cat spambase.test spambase.train > spambase.data
./nfold.py --folds=5 --data=spambase.data --spec=spambase.spec \
           --rounds=500 --tree=ADD_ALL --generate
</pre>

<p>
This will create the directory cvdata-MONTH-DAY-HOUR-MINUTE-SECOND
containing a directory ADD_ALL, files trial<em>K</em>.train and
trial<em>K</em>.test where <em>K</em> ranges from 1 to N, and
trial.spec.  The ADD_ALL directory will contain all the runtime
information for each trial.  To edit other runtime, parameters of
JBoost (e.g. memory size, number of threads, amount of output via -a
switch, etc), edit the <code>learner</code> function in nfold.py.
</p>

</div>

<div align="left" class="section">
<h2 class="section_heading" ><a name="multiclass">Dealing with Multiclass Problems</a></h2>
<p>
In version 2.0 JBoost stopped internally supporting multiclass problems.  Instead, a python script is now offered that splits a multiclass problem into many binary problems.  It can be found in the scripts folder in the jboost root directory.</p>

<p>First, it estimates via cross-validation the test error of a one-vs-all set of classifiers (the label of a test example is given by the one-vs-all classifier that gives it the largest score).  As a side product, a confusion matrix C can be built where C(i,j) is the number of test examples with label i that were classified as j.  From this information, it attempts to split the classes into two groups via spectral clustering on the symmetrized confusion matrix.  A classifier that discriminates between these two groups is learned, and the process repeated on the resulting two child nodes.  This produces a tree of classifiers where singleton labels are at the leaves.  The test error of this classifier is also reported.</p>

<p>The usage of the script is as follows.  First, simply alter your spec file to include all the labels for your multiclass problem.  Then call MultiClassHierarchical.py with like the following:</p>

<pre>
./MultiClassHierarchical.py --S=./datasets/iris/iris --numRounds=10 --ATreeType=ADD_ROOT_OR_SINGLES
</pre>

<p>Here is an example output from the script on the iris UCI dataset:</p>

<pre>
*********New Node**********
ClassList:
 ['Iris-versicolor', 'Iris-virginica', 'Iris-setosa']
Confusion Matrix:
 46    4    0
  6   44    0
  0    0   50
ZeroLabels:
 ['Iris-versicolor', 'Iris-virginica']
OneLabels:
 ['Iris-setosa']
LLB: acc +/- std, (iters) = 0.000000 +/- 0.000000, (10)
ADA: acc +/- std, (iters) = 0.000000 +/- 0.000000, (10)
   *********New Node**********
   ClassList:
    ['Iris-versicolor', 'Iris-virginica']
   Confusion Matrix:
    47    3
     4   46
   ZeroLabels:
    ['Iris-versicolor']
   OneLabels:
    ['Iris-virginica']
   LLB: acc +/- std, (iters) = 0.070000 +/- 0.050990, (10)
   ADA: acc +/- std, (iters) = 0.070000 +/- 0.050990, (10)

************************SUMMARY************************
LLB 1 vs All Classification Error:               0.0667
ADA 1 vs All Classification Error:               0.0667
LLB Spectral Cluster Tree Classification Error:  0.0467
ADA Spectral Cluster Tree Classification Error:  0.0467
</pre>
<p>Here the tree structure of classifiers is shown.  Shown are: the confusion matrix built by finding one-vs-all classifiers, the two clusterings of labels, and the resulting classifier built by AdaBoost and LogLossBoost to separate the two clusters.  Children nodes are indicated by indentations. Finally at the end the overall accuracy of each classifier on the cross validated test sets are shown.</p>

<p>The full list of options is given by:</p>
<pre>
Usage: MultiClassHierarchical.py
	--S=&lt;stem&gt;           path to stem name -- expecting &lt;stem&gt;.data and &lt;stem&gt;.spec to be there
	--numRounds=N        number of rounds to run each classifier by
	--folds=N            number of folds to use in cross-validation. 
                             Default: 5
	--rb                 RobustBoost will be run. By default this is off.
                             WARNING: Can take a very long time.
	--ATreeType=&lt;tree&gt;   ADD_ROOT, ADD_SINGLES, ADD_ROOT_OR_SINGLES, or ADD_ALL 
                             ADTree types (ADD_ROOT by default)
</pre>

<p>This script is just for testing purposes.  Unfinished features include using the various output functions that JBoost can offer, e.g in Python or Matlab, and combine them into a multiclass combined classifier.
</div>

<div align="left" class="section">
<h2 class="section_heading" ><a name="rbsearch">Searching for Good RobustBoost Parameter Settings</a></h2>
<p>
We have developed a script that we recommend for searching for the optimal setting of parameters in RobustBoost.  It can be found in the scripts/MultiClass folder from the JBoost root directory.</p>

<p>It first fixes sigma_f at 1.0, which is typically a good setting for most problems in our experience.  Then, for each of theta in {0,1,2,4,8,16,32,64} it tries to find the smallest epsilon for which RobustBoost can still get to t=1.0 within the specified number of boosting iterations.  It uses a binary searching procedure on epsilon for each theta.</p>

<p>The usage of the script for a binary problem is as follows:</p>

<pre>
./RobustBoostSearch.py --S=./datasets/breast-w/breast-w --numRounds=200 --ATreeType=ADD_ROOT_OR_SINGLES
</pre>

<p>Here is an example output from the script on the breast-w UCI dataset:</p>

<pre>
******FINAL RESULTS*****

RB: acc +/- std, (epsilon,theta,sigma_f) = 0.038680 +/- 0.024394, (0.00,2.00,1.00)
</pre>

<p>Shown are the best setting of sigma_f, theta and epsilon with the test accuracy according to a cross-validation.</p> 

<p>The full list of options is given by:</p>
<pre>
Usage: RobustBoostSearch.py
	--S=&lt;stem&gt;           path to stem name -- expecting &lt;stem&gt;.data and &lt;stem&gt;.spec to be there
	--numRounds=N        number of rounds to run each classifier by
	--folds=N            number of folds to use in cross-validation. 
                             Default: 5
	--ATreeType=&lt;tree&gt;   ADD_ROOT, ADD_SINGLES, ADD_ROOT_OR_SINGLES, or ADD_ALL 
                             ADTree types (ADD_ROOT by default)
</pre>
</div>


<div align="left" class="section">
<h2 class="section_heading" ><a name="boost_format"><code>boosting.info</code> Output Format </a></h2>
<p>
This is the file that is parsed by most of the visualization scripts.
We provide this brief description of the fields so that people can
write their own parsing scripts.
</p>

<p>
The file has as much information as specified by the <code>-a</code>
switch (see JBoost usage).  Each iteration that is outputted, there is
a preamble line describing the parameters associated with a given
booster.  This is followed by an enumeration of all examples,
accompanied by their ID, margin, score, weight, potential, and label.
This list may expand in the future.
</p>

<p>
The format is most easily understood via a demonstration:
</p>


<pre>
iteration=5: elements=3681: boosting_params=None (AdaBoost):
0: -0.12695: -0.12695: 0.18725: 0.0: +1: 
1: 0.53953: -0.53953: 0.03180: 0.0: -1: 
. . .
</pre>

<p>
There are two examples shown, separated by a the system dependent
newline character(s).  We go over the first example:

<ul> 

<li>All fields are separated by a colon ':' character.</li>

<li> The example has an internal identifier of 0. </li>

<li> The margin is -0.12695.  This is because it has a +1 label but
it's current score is -0.12695. </li>

<li> The weight of the example is 0.18725. </li>

<li> The potential loss of the example is 0.0.  This option is only
used by BrownBoost, NormalBoost and other potential based boosting
algorithms.  LogLossBoost and AdaBoost potential functions can be
derived from the weights and are not stored internally.  </li>

<li>The label is +1. This is the internal label given by JBoost,
your label may be "-1", "Alpha Helix", etc.  </li>

</ul>

</p>

<p>
The above example is for binary classification tasks, for multiclass
classification, JBoost currently uses a 1-vs-all approach.  This means
that if there are 20 possible classes for an instance, then each
instance is expanded into 20 instances of binary classification.
</p>

<pre>
iteration=0: elements=6: boosting_params=BrownBoost r=1.4000 s=1.3927 : 
0: -0.174: -0.174,-0.174,-0.174: 0.173,0.346,0.173: 0.030,0.072,0.030: -1,+1,-1:
. . .
</pre>

<p>
In the above example, we see that the ID is 0 and the margin is
-0.174.  However, each class now has it's own weight, potential, and
label separated by commas.  Also, since we used BrownBoost, we can now
see the parameters passed to the booster.  Note that the weight (and
potential) are higher for the second   class.  This is because the other
two classes are correctly scored as "-1," whereas the second class has
label "+1" in this example.
</p>

</div>

<!--
<div align="left" class="section">
<h2 class="section_heading" ><a name=""></a></h2>
<p>
</p>
</div>
-->


<div align="left" class="section">
<h2 class="section_heading" ><a name="links">Links on Boosting</a></h2>

To learn more about general boosting techniques, there are a variety
of sources.  Here are some links to get started:

<ul>
<li>
<a href="http://en.wikipedia.org/wiki/Alternating_decision_tree">ADTrees on Wikipedia</a>
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Boosting">Boosting on Wikipedia</a>
</li>
<li>
<a href="http://www.boosting.org">Boosting.org</a>
</li>
<li>
<a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost on Wikipedia</a>
</li>
<li>
<a href="http://en.wikipedia.org/wiki/BrownBoost">BrownBoost on Wikipedia</a>
</li>
<li>
<a href="http://www.cs.ucsd.edu/~yfreund/adaboost/index.html">AdaBoost Applet Demonstration</a>
</li>
<li>
<a href="http://www.cs.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">AdaBoost: A Short Introduction</a>
</li>
</ul>

</div>








</td>

</tr>

</table>

<hr />


<p>
 <a href="http://jigsaw.w3.org/css-validator/">
  <img style="border:0;width:88px;height:31px"
       src="http://jigsaw.w3.org/css-validator/images/vcss" 
       alt="Valid CSS!">
 </a>
 <a href="http://validator.w3.org/check?uri=referer">
  <img style="border:0;width:88px;height:31px"
       src="http://www.w3.org/Icons/valid-html401"
       alt="Valid HTML 4.01 Transitional" height="31" width="88"></a>

<!-- <a href="http://sourceforge.net/">
  <img style="border:0;width:88px;height:31px"
       src="http://sflogo.sourceforge.net/sflogo.php?group_id=195659"
       alt="Source Forge"</a> -->

<a href="http://sourceforge.net"><img src="http://sflogo.sourceforge.net/sflogo.php?group_id=195659&amp;type=1" width="88" height="31" border="0" alt="SourceForge.net Logo" /></a>
<!--
 <a href="http://jboost.sourceforge.net/">
  <img style="border:0;width:88px;height:31px"
       src="http://sflogo.sourceforge.net/sflogo.php?group_id=195659"
       alt="JBoost on Source Forge"</a>
-->
</p>

<p>
This page last modified Thursday, 18-Jun-2009 03:10:28 UTC
</p>

</body>
</html>


